<a href="https://www.kaggle.com/code/alibahadorani/train-bloom-1b7-tokenizer/edit"><img src="https://kaggle.com/static/images/open-in-kaggle.svg" alt="Open In Kaggle"></a>
<a href="https://huggingface.co/datasets/ali619/corpus-dataset-normalized-for-persian-and-english"><img src="https://img.shields.io/badge/Dataset_on-%F0%9F%A4%97-white" alt="Model on HF">

# Create a tokenizer for bigScience-bloom-1B7 model with *Persian-English Corpus*

### You can check all the steps in the notebook or you can run the notebook on *kaggle* if you want to test it out

This tokenizer is trained on more than **3.2** million rows of data.

* To get more info about the data I used, check out dataset page on [ðŸ¤—Face Datasets](https://huggingface.co/datasets/ali619/corpus-dataset-normalized-for-persian-and-english)